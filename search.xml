<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Distributed Deep Learning with Horovod</title>
    <url>/2019/12/09/Horovod/</url>
    <content><![CDATA[<h1 id="Distributed-Deep-Learning-with-Horovod"><a href="#Distributed-Deep-Learning-with-Horovod" class="headerlink" title="Distributed Deep Learning with Horovod"></a>Distributed Deep Learning with Horovod</h1><p>看完了一波分布式深度学习原理，并尝试使用pytorch实现本地两台gpu服务器(各4张卡)之间的通信，结果屡试屡败，太有挫败感，不得已弃之使用人人称道的Horovod。也是在这个时候比较系统地学习了一遍docker，并使用docker成功搭建好了基于Horovod的分布式深度学习环境。下面首先介绍分布式深度学习多机训练机制，后再介绍如何使用Horovod搭建多机训练平台。</p>
<h2 id="分布式深度学习"><a href="#分布式深度学习" class="headerlink" title="分布式深度学习"></a>分布式深度学习</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>模型训练时间的长短主要取决于数据集的大小、模型的复杂度(模型的存储和计算量)和硬件资源的利用。而数据集的大小、模型的复杂度固定的情况下，如何充分利用集群/硬件资源来加速模型的训练，这就是分布式深度学习所要研究的内容。</p>
<h4 id="模型复杂度的提升"><a href="#模型复杂度的提升" class="headerlink" title="模型复杂度的提升"></a>模型复杂度的提升</h4><p>无论是CV、NLP还是其它涉及到深度学习的领域，往往是模型的复杂度越高，模型的效果越好。所以对于某些偏向于追求效果的研究和应用，现阶段的模型复杂度都相当高，比如GPT-2的1.5 billion的参数。</p>
<h4 id="数据集的增大"><a href="#数据集的增大" class="headerlink" title="数据集的增大"></a>数据集的增大</h4><p>BIG DATA时代，很多任务中的数据集往往都是特别大，甚至愈来愈大。比如推荐领域，大公司每天的日志量可能都是T级别及以上。</p>
<h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>当前的分布式深度学习主要包含如下模块</p>
<ul>
<li>数据与模型划分：模型并行vs<strong>数据并行</strong></li>
<li>单机优化：优化算法(略)</li>
<li>通信机制：<strong>网络拓扑</strong>(ParameterServer vs Allreduce), <strong>通信步调</strong>(同步更新vs异步更新)</li>
<li>模型聚合：对单机优化的模型参数/梯度进行汇总，更新新的参数(略)</li>
</ul>
<h3 id="模型并行vs数据并行"><a href="#模型并行vs数据并行" class="headerlink" title="模型并行vs数据并行"></a>模型并行vs数据并行</h3><p><img src="/2019/12/09/Horovod/model_data_parallelism.png" alt="model_data_parallelism"></p>
<p>当模型过大无法在单机上存储时，会采用模型并行将模型分割到不通机器上，但多数情况下，一块常规显卡还是能承受住模型的存储开销的，目前训练数据量过大导致的模型训练速度慢才是分布式机器学习领域的主要矛盾。所以大多数情况下还是使用数据并行的方式，每台机器上平均分配一个batch的数据量。</p>
<h3 id="同步更新vs异步更新"><a href="#同步更新vs异步更新" class="headerlink" title="同步更新vs异步更新"></a>同步更新vs异步更新</h3><p><img src="/2019/12/09/Horovod/syn_update.jpg" alt="syn"></p>
<p><img src="/2019/12/09/Horovod/asyn_update.jpg" alt="syn"></p>
<p>同步更新符合梯度下降的规则，但是更新的速度取决于运算最慢的那台机器。</p>
<p>异步更新虽然不用等，但是会出现梯度延时的问题。<strong>如何设计高效的异步更新算法，这也是目前研究的一个热点。</strong></p>
<h3 id="Parameter-Server-vs-Ring-Allreduce"><a href="#Parameter-Server-vs-Ring-Allreduce" class="headerlink" title="Parameter Server vs Ring Allreduce"></a>Parameter Server vs Ring Allreduce</h3><p>在分布式深度学习中，目前主流的通信拓扑结构是PS和Ring Allreduce。拿PS来说，至少经历了三次版本的迭代，这里不打算展开，主要介绍这两种结构的基本思想。</p>
<p>首先看到一个简化的PS的结构：</p>
<p><img src="/2019/12/09/Horovod/ps.png" alt="ps" style="zoom:50%;"></p>
<p>图中GPU 0是server node，其余为worker node。</p>
<p><strong>对于数据并行的同步更新训练过程，所有GPU上开始都存有模型的副本，每一轮迭代所有的worker需要将各自backward得到的参数梯度传递给server来做平均、更新，server更新好参数后再将其复制给所有的worker。</strong></p>
<p><strong>两次传递过程的通信量分别是模型参数及其梯度</strong>。从上图可以明显看到，PS的瓶颈出现在<strong>server与其余worker之间的通信</strong>，在每一轮的迭代中，send和receive的一方总是单个GPU，如此大的通信量都是要和单个GPU打交道，而单个GPU的带宽是很有限的，随着worker数目增加，通信的开销将会越大。举例而言，假设需要训练的模型有300w参数，每个参数是4bytes，那么参数的memory就大约1.2G，最后再假设带宽是1G/s。那么当PS中只有两个node时，每次迭代大概需要1.2s。当PS中有9个worker一个server时，这个开销就至少是10.8s。也即说明随着worker的增加，通信的开销呈线性增长。</p>
<p>为了解决这个问题，百度研究院将HPC中的技术引入了进来，也就是Ring Allreduce。</p>
<p><img src="/2019/12/09/Horovod/ringallreduce.png" alt="ring" style="zoom:50%;"></p>
<p><strong>Ring Allreduce中每一个结点都有一个左结点和右结点，并且每一个结点只能从左节点recv，向右结点send。</strong></p>
<p>Ring Allreduce包含两个步骤：先Scatter-Reduce后Allgather，最终目的是在每个结点上都得到参数梯度的平均。</p>
<p><strong>Scatter Reduce</strong> 首先在每个结点上将模型参数梯度划分成N个一致的chunks，其中N为Ring上GPU的个数。这样所有结点上模型的参数梯度组合形成一个方阵。同理，每个chunk都相应有自己的右chunk和左chunk，模型参数梯度划分成N个chunk会形成N个chunk ring。</p>
<p>然后从参数梯度方阵对角线上的chunk开始，每个chunk向其右chunk send一份自己的备份，同时右chunk需要recv这个备份并加和。之后经过N-1次迭代，矩阵的右-1对角线chunk和左下角chunk会得到各自所在的chunk ring上所有chunk之和。</p>
<p><img src="/2019/12/09/Horovod/scatter_reduce.gif" alt="scatter_reduce" style="zoom:50%;"></p>
<p><strong>All Gather</strong> 在Scatter Reduce的基础上，每个GPU上都有一个chunk是其所在chunk ring上的参数梯度之和。所以只需要将这份chunk在其所在的ring上传播N-1次，那么每个GPU上就得到了所有结点上参数梯度之和，这也是All Gather所做的工作。之后各结点对参数梯度求平均即得到整个batch的平均梯度。</p>
<p><img src="/2019/12/09/Horovod/all_gather.gif" alt="all_gather" style="zoom:50%;"></p>
<h2 id="Horovod"><a href="#Horovod" class="headerlink" title="Horovod"></a>Horovod</h2><p>Horovod是一款分布式训练框架，可用于各大深度学习框架如TF、Pytorch、Keras和MXNet。虽然每一个深度学习框架本身也实现了各自的分布式训练功能，但实作中发现效果并不理想。如下是tensorflow提供的分布式训练效果：</p>
<p><img src="/2019/12/09/Horovod/horovod-1.png" alt="horovod vs tf" style="zoom:67%;"></p>
<p>可以很明显看到，tensorflow的加速比随着gpu的增加严重下滑,128卡的情况下居然较Ideal损耗了一半的性能。</p>
<p>horovod就是针对这一问题才出现的，如下是horovod与tensorflow的对比：</p>
<p><img src="/2019/12/09/Horovod/horovod-2.png" alt="horovod win tensorflow" style="zoom:67%;"></p>
<p>可以看到horovod相较于Ideal性能损耗较小，想必这也是horovod在市面上留有一席之地的根本原因。</p>
<h3 id="Horovod-配置与使用"><a href="#Horovod-配置与使用" class="headerlink" title="Horovod 配置与使用"></a>Horovod 配置与使用</h3><p>本文是基于docker环境的horovod配置，需要的主要原材料有：</p>
<ul>
<li>docker</li>
<li>nvidia-docker</li>
</ul>
<h4 id="docker-amp-nvidia-docker安装"><a href="#docker-amp-nvidia-docker安装" class="headerlink" title="docker&amp;nvidia-docker安装"></a>docker&amp;nvidia-docker安装</h4><p>docker</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">sudo add-apt-repository \</span><br><span class="line">   <span class="string">"deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu \</span></span><br><span class="line"><span class="string">   <span class="variable">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">   stable"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install docker-ce</span><br></pre></td></tr></table></figure>
<p>nvidia-docker</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \</span><br><span class="line">  sudo apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | \</span><br><span class="line">  sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br><span class="line">sudo pkill -SIGHUP dockerd</span><br></pre></td></tr></table></figure>
<p>安装curl</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install curl</span><br></pre></td></tr></table></figure>
<p>需要注意，可能会出现如下错误：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl: symbol lookup error: /opt/anaconda3/bin/../lib/libcurl.so.4: undefined symbol: sslv2_client_method</span><br></pre></td></tr></table></figure>
<p>这是因为anaconda也带了openssl和curl，并且anaconda在环境变量里，所以使用curl的时候会用到anaconda中的curl，笔者在实验中发现这里的curl和openssl有问题，出现了上方的错误。我所采取的解决办法是将anaconda中的openssl和curl改名，然后重装openssl和curl即可。</p>
<h3 id="Horovod-in-Docker"><a href="#Horovod-in-Docker" class="headerlink" title="Horovod in Docker"></a>Horovod in Docker</h3><p>为了简化GPU机器上的安装过程，Horovod项目发布了Dockerfile，方便我们使用docker来快速配置环境。该容器在/examples目录中包含了Horovod的使用示例。用户也可以在<a href="https://hub.docker.com/r/horovod/horovod/tags" target="_blank" rel="noopener">DockerHub</a>中直接下载预先构建好的容器。</p>
<h4 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h4><p>在building之前，你可以根据需求对Dockerfile进行修改，比如不同的CUDA、TensorFlow和python版本，也可以在安装的命令中指定源，解决下载慢的问题。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mkdir horovod-docker-gpu</span><br><span class="line">$ wget -O horovod-docker-gpu/Dockerfile https://raw.githubusercontent.com/horovod/horovod/master/Dockerfile.gpu</span><br><span class="line">$ docker build -t horovod:latest horovod-docker-gpu</span><br></pre></td></tr></table></figure>
<h4 id="Running-on-a-single-machine"><a href="#Running-on-a-single-machine" class="headerlink" title="Running on a single machine"></a>Running on a single machine</h4><p>镜像构建好之后，使用 <code>nvidia-docker</code> 来启用容器。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nvidia-docker run -it horovod:latest</span><br><span class="line">root@c278c88dd552:/examples<span class="comment"># horovodrun -np 4 -H localhost:4 python keras_mnist_advanced.py</span></span><br></pre></td></tr></table></figure>
<p>如果不是以<code>privileged</code>模式运行容器，可能会出现如下问题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[a8c9914754d2:00040] Read -1, expected 131072, errno = 1</span><br></pre></td></tr></table></figure>
<p>可以无视之。</p>
<h4 id="Running-on-a-multiple-machines"><a href="#Running-on-a-multiple-machines" class="headerlink" title="Running on a multiple machines"></a>Running on a multiple machines</h4><p>想要在多机上进行训练，最关键的问题是多机之间的通信。下面采用SSH来进行多机之间的免密通信。</p>
<p>首先，我们通过共享文件的方式，使每台机器下的<code>/mnt/share/ssh</code>与容器中的<code>/root/.ssh</code>进行绑定，并且在<code>/mnt/share/ssh</code>中，配置好<code>id_rsa</code>和<code>authorized_keys</code> 对来允许免密登录。</p>
<p>Primary worker:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">host1$ nvidia-docker run -it --privileged --network=host -v /mnt/share/ssh:/root/.ssh horovod:latest</span><br><span class="line">root@c278c88dd552:/examples<span class="comment">#/usr/sbin/sshd -p 12345 &amp; horovodrun --start-timeout 1000 --verbose -np 16 -H host1:4,host2:4,host3:4,host4:4 -p 12345 python keras_mnist_ad</span></span><br></pre></td></tr></table></figure>
<p>Secondary workers:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">host2$ nvidia-docker run -it --network=host -v /mnt/share/ssh:/root/.ssh horovod:latest \</span><br><span class="line">    bash -c <span class="string">"/usr/sbin/sshd -p 12345; sleep infinity"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">host3$ nvidia-docker run -it --network=host -v /mnt/share/ssh:/root/.ssh horovod:latest \</span><br><span class="line">    bash -c <span class="string">"/usr/sbin/sshd -p 12345; sleep infinity"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">host4$ nvidia-docker run -it --network=host -v /mnt/share/ssh:/root/.ssh horovod:latest \</span><br><span class="line">    bash -c <span class="string">"/usr/sbin/sshd -p 12345; sleep infinity"</span></span><br></pre></td></tr></table></figure>
<p>以下是笔者成功在两台机器上运行成功的效果图：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mpirun --allow-run-as-root --tag-output -np 6 -H w2:4,w1:2 -<span class="built_in">bind</span>-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca plm_rsh_args <span class="string">"-p 12345"</span> -mca btl_tcp_if_include ppp0 -x NCCL_SOCKET_IFNAME=ppp0 -x CUDNN_VERSION -x LS_COLORS -x LD_LIBRARY_PATH -x MXNET_VERSION -x HOSTNAME -x NVIDIA_VISIBLE_DEVICES -x NCCL_VERSION -x PWD -x HOME -x TENSORFLOW_VERSION -x PYTORCH_VERSION -x LIBRARY_PATH -x TORCHVISION_VERSION -x TERM -x CUDA_PKG_VERSION -x CUDA_VERSION -x NVIDIA_DRIVER_CAPABILITIES -x PYTHON_VERSION -x SHLVL -x NVIDIA_REQUIRE_CUDA -x PATH -x _ -x HOROVOD_STALL_CHECK_TIME_SECONDS -x HOROVOD_STALL_SHUTDOWN_TIME_SECONDS -x HOROVOD_NUM_NCCL_STREAMS -x HOROVOD_MLSL_BGT_AFFINITY python keras_mnist_advanced.py</span><br></pre></td></tr></table></figure>
<p><img src="/2019/12/09/Horovod/two_machine_workable.png" alt="image-20191204101727329" style="zoom:50%;"></p>
<p>由于笔者资源有限，只有两台机器，而且机器之间的网络传输最高速率只有100M，所以运行example的code并没能体现出多机的价值，性能还不及单机单卡。</p>
<h3 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h3><p>再多机配置的过程中发现许多问题，因为时间久远，记录下比较深刻的一些。</p>
<h4 id="1-数据传输验证"><a href="#1-数据传输验证" class="headerlink" title="1. 数据传输验证"></a>1. 数据传输验证</h4><p>之前说到了，多机训练最关键的是在多机之间的通信。如果master与slave之间的数据传输出了问题，铁定是不会成功的。这个时候需要我们对传输的有效性进行一步步的验证。</p>
<h5 id="1-1-ssh免密验证"><a href="#1-1-ssh免密验证" class="headerlink" title="1.1 ssh免密验证"></a>1.1 ssh免密验证</h5><p>验证非常简单，在master(w2)中使用<code>ssh w1</code>看是否能够直接登录，可以的话就说明ssh的免密配置成功，否则重新配置一遍。</p>
<p><img src="/2019/12/09/Horovod/ssh_config.png" alt="ssh_config"></p>
<h5 id="1-2-scp远程传输测试"><a href="#1-2-scp远程传输测试" class="headerlink" title="1.2 scp远程传输测试"></a>1.2 scp远程传输测试</h5><p>虽然能够免密成功，但可能会出现传输不了文件的情况。这个时候可以在master机器和slave机器之间使用scp命令测试是否能够在多台机器之间完成数据传输。笔者就在实验中发现scp传输失败，网上尝试了贼多方法，最终发现是需要修改MTU的值，默认是1500。ppp0是我拨号上网的接口名。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ip link <span class="built_in">set</span> ppp0 mtu 1492</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255" target="_blank" rel="noopener">Training Neural Nets on Larger Batches</a></p>
<p><a href="http://fyubang.com/2019/07/08/distributed-training/" target="_blank" rel="noopener">单机多卡的正确打开方式（一）：理论基础</a></p>
<p><a href="http://andrew.gibiansky.com/" target="_blank" rel="noopener">Bringing HPC Techniques to Deep Learning</a></p>
<p><a href="https://www.cnblogs.com/bingmang/p/9813686.html" target="_blank" rel="noopener">docker/nvidia-docker 安装教程</a></p>
<p><a href="https://github.com/horovod/horovod/blob/master/docs/docker.rst" target="_blank" rel="noopener">Horovod in Docker</a></p>
]]></content>
      <categories>
        <category>Coding</category>
        <category>Distributed Deep Learning</category>
      </categories>
      <tags>
        <tag>Distributed Deep Learning</tag>
        <tag>Horovod</tag>
      </tags>
  </entry>
  <entry>
    <title>经典无监督方法</title>
    <url>/2019/11/28/%E7%BB%8F%E5%85%B8%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>本文的目的是回顾经典的无监督方法，方便笔者在失忆的时候迅速回忆起来。</p>
<p>目前包含如下内容：</p>
<ul>
<li>PCA</li>
<li>K-Means Clustering</li>
<li>NMF</li>
<li><p>Spectral Clustering</p>
<a id="more"></a> 
</li>
</ul>
<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>可以从两个不同的角度出发，推导出PCA的优化目标。</p>
<h3 id="1-方差最大化"><a href="#1-方差最大化" class="headerlink" title="1.方差最大化"></a>1.方差最大化</h3><blockquote>
<p><strong>LAMA1</strong> 任意矩阵$X \in R^{D \times n}$, 若满足$\sum_{i=1}^n x_i = \boldsymbol 0$, 则 $\frac{1}{n-1} XX^T$ 为 $X$ 的协方差矩阵，$Tr(XX^T)=\Vert X \Vert_F^2$ 为 $X$ 的方差。 </p>
</blockquote>
<p> 把 $X$ 按行向量的形式写，第 $i$ 行用 $(x^i)^T$ 表示:</p>
<script type="math/tex; mode=display">
X =
\left[
\begin{matrix}
 (x^1)^T      \\
 (x^2)^T      \\
 \vdots \\
 (x^D)^T      \\
\end{matrix}
\right]</script><p>那么</p>
<script type="math/tex; mode=display">
XX^T =
\left[
\begin{matrix}
(x^1)^Tx^1 & (x^1)^Tx^2 & \cdots & (x^1)^Tx^D \\
(x^2)^Tx^1 & (x^2)^Tx^2 & \cdots & (x^2)^Tx^D \\
\vdots & \vdots & \ddots & \vdots \\
(x^D)^Tx^1 & (x^D)^Tx^2 & \cdots & (x^D)^Tx^D \\
\end{matrix}
\right]</script><p>第 $i$ 个维度的方差$Var(x^i)$满足</p>
<script type="math/tex; mode=display">
\begin{align}
(n-1) \times Var(x^i)
& = \sum\limits_{j=1}^n (x_{ij}-\frac{1}{n}\sum\limits_{j=1}^n x_{ij})^2 \\
& = \sum\limits_{j=1}^n x_{ij}^2 \ (x^i均值为0) \\
& = (x^i)^Tx^i \\
\end{align}</script><p>$i,j$ 两个维度的协方差$Cov(x^i, x^j)$满足</p>
<script type="math/tex; mode=display">
\begin{align}
(n-1) \times Cov(x^i, x^j)
& = \sum\limits_{k=1}^n (x_{ik}-\frac{1}{n}\sum\limits_{k=1}^n x_{ik}) (x_{jk}-\frac{1}{n}\sum\limits_{k=1}^n x_{jk}) \\
& = \sum\limits_{k=1}^n x_{ik}x_{jk} \\
& = (x^i)^Tx^j \\
\end{align}</script><p>由此可知，$\frac{1}{n-1}XX^T$ 即为 $X$ 的协方差矩阵。</p>
<p>下面用 $W \in R^{D \times d} $ 描述投影变换， $Y=W^TX$ 表示投影之后的样本空间，很明显 $Y$ 也是中心化的，那么 $\frac{1}{n-1}YY^T$ 也表示投影后的样本的协方差矩阵。</p>
<p>我们希望样本在每一个维度投影后应尽可能多地保留原始空间的信息，同时为了防止信息在投影之后冗余，我们也希望每一个维度之间相互正交。于是有：</p>
<script type="math/tex; mode=display">
\begin{align}
& \max\limits_{W^TW=I} Tr(Y^TY) \\
& => \max\limits_{W^TW=I}Tr(W^TXX^TW) \\
\end{align}</script><h3 id="2-重构误差最小化"><a href="#2-重构误差最小化" class="headerlink" title="2.重构误差最小化"></a>2.重构误差最小化</h3><script type="math/tex; mode=display">
\begin{align}
& \min\limits_{rank(Z)=d}\|X-Z\|_F^2 \\
& => \min\limits_{U^TU=I,V}\|X-UV^T\|_F^2\ (Z满秩分解)\\
& => \min\limits_{U^TU=I}\|X-UU^TX\|_F^2\ (对V求导为0消去V)\\
& => \min\limits_{U^TU=I}Tr(X^TX-X^TUU^TX) \\
& => \max\limits_{U^TU=I}Tr(U^TXX^TU)
\end{align}</script><h1 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h1><script type="math/tex; mode=display">
\begin{align}
& \min_C \sum\limits_{j=1}^k \sum\limits_{x_i \in C_j} \|x_i-c_j\|_2^2 \\
& => \min_{G \in Ind, C} \sum\limits_{i=1}^n \sum\limits_{j=1}^k \|x_i-c_j\|_2^2 g_{ij}\\
& => \min_{G \in Ind, C} \|X-CG^T\|_F^2 \\
& => \min_{G^TG=I,C} \|X-CG^T\|_F^2 \ (\boldsymbol{relax \ G}) \\
& => \min_{G^TG=I,C} \|X-XGG^T\|_F^2 \\
& => \min_{G^TG=I} Tr(X^TX-G^TX^TXG) \\
& => \max_{G^TG=I} Tr(G^TX^TXG)
\end{align}</script><h1 id="NMF"><a href="#NMF" class="headerlink" title="NMF"></a>NMF</h1><script type="math/tex; mode=display">
\min_{F \geq 0,G \geq 0}\|X-FG^T\|_F^2</script><h3 id="1-SymNMF"><a href="#1-SymNMF" class="headerlink" title="1.SymNMF"></a>1.SymNMF</h3><script type="math/tex; mode=display">
\begin{align}
& \min_{F \geq 0}\|X-FF^T\|_F^2 \\

\end{align}</script><h1 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h1><p>矩阵 $A$ 的特征值的全体，称为 $A$ 的谱<strong>Spectra</strong>，记为 $\lambda(A)$</p>
<h3 id="0-Laplacian-Matrix"><a href="#0-Laplacian-Matrix" class="headerlink" title="0.Laplacian Matrix"></a>0.Laplacian Matrix</h3><h4 id="0-1-Undirected-Graph-amp-affinity-matrix-amp-degree-matrix"><a href="#0-1-Undirected-Graph-amp-affinity-matrix-amp-degree-matrix" class="headerlink" title="0.1 Undirected Graph &amp; affinity matrix &amp; degree matrix"></a>0.1 Undirected Graph &amp; affinity matrix &amp; degree matrix</h4><p>一个图 $G$ 由顶点集 $V$ 和边集 $E$ 构成，记为 $G(V,E)$</p>
<p>定义任意两个顶点 $v_i$ 和 $v_j$ 之间的权重 $w_{ij}$ ，无向图中 $w_{ij}=w_{ji}$ ，所有的权重共同构成<strong>邻接矩阵 $W$</strong></p>
<p>顶点 $v_i$ 的度 $d_i=\sum\limits_{j=1}^n w_{ij}$ ，所有顶点的度构成的对角阵共同构成<strong>度矩阵 $D$</strong></p>
<p>另外，对于任意 $A \in V$ , 我们定义 $|A|$ 表示 <strong>$A$ 中元素的个数</strong>；定义 $vol(A)=\sum\limits_{i \in A} d_i$ 表示 <strong>$A$ 中所有点的总度数</strong></p>
<h4 id="0-2-similarity-matrix"><a href="#0-2-similarity-matrix" class="headerlink" title="0.2 similarity matrix"></a>0.2 similarity matrix</h4><p>一般无法直接获得邻接矩阵，需要人为设计相似度矩阵来近似邻接矩阵。</p>
<p>设计的基本原则是</p>
<blockquote>
<p>距离越近的两个样本点之间的相似度越大，距离越远相似度越小。一般大于0。</p>
</blockquote>
<p>实际操作中，最常用的是使用<strong>高斯核函数</strong>来定义权重：</p>
<script type="math/tex; mode=display">
w_{ij} = w_{ji} = exp(-\frac{\|x_i-x_j\|_2^2}{2 \sigma^2})</script><h4 id="0-3-Laplacian-Matrix"><a href="#0-3-Laplacian-Matrix" class="headerlink" title="0.3 Laplacian Matrix"></a>0.3 Laplacian Matrix</h4><script type="math/tex; mode=display">
L=D-W</script><p>它有如下性质：</p>
<ol>
<li><p>$L$ 是对称矩阵；</p>
</li>
<li><p>$L$ 的特征值都是实数；</p>
</li>
<li><p>除了对角线元素非负，其余都非正。</p>
</li>
<li><p>对于任意向量 $f \in R^n$, 有</p>
<script type="math/tex; mode=display">
\begin{align}
f^TLf
& = f^TDf - f^TWf \\
& = \sum\limits_{i=1}^n d_if_i^2 - \sum\limits_{i=1}^n \sum\limits_{j=1}^n w_{ij}f_if_j \\
& = \sum\limits_{i=1}^n \sum\limits_{j=1}^n w_{ij}f_i^2 - \sum\limits_{i=1}^n \sum\limits_{j=1}^n w_{ij}f_if_j \\
& = \frac{1}{2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n w_{ij} (f_i^2-2f_if_j+f_j^2) \\
& = \frac{1}{2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n w_{ij}(f_i-f_j)^2 \\
\end{align}</script></li>
<li><p>$L$ 半正定;</p>
</li>
<li><p>$L\boldsymbol1=\boldsymbol 0, \boldsymbol1^TL\boldsymbol 1=0$;</p>
</li>
<li><p>$L$ 有 $n$ 个非负实特征值，且 $0=\lambda_1\leq \lambda_2 \leq \dots \leq \lambda_n$. ( $L \boldsymbol 1 = \boldsymbol 0$ )</p>
</li>
<li><p><strong>$L$ 的 $0$ 特征值的重数等于图中连通分量的个数，且每个特征向量为对应连通分量的指示向量</strong>。</p>
<p><em>Proof</em>：</p>
<ul>
<li>当连通分量个数为$1$; 假设 $0$ 特征值对应的特征向量为 $f$, 即$Lf=0$，所以$f^TLf=0$，又因为$ w_{ij} &gt; 0 $, 所以必有 $f_i = f_j = 1 $。也即0特征值只有一个对应的特征向量，所以$0$特征值重数为$1$。</li>
<li>当连通分量个数为 $k$; 不失一般性，将 $W$ 根据连通性重排成块对角矩阵，同样对于 $L$ 有相应的块对角结构。</li>
</ul>
</li>
</ol>
<script type="math/tex; mode=display">
L =
   \left[
   \begin{matrix}
   L_1  \\
   & L_2  \\
   & & \ddots \\
   & & & L_k \\
   \end{matrix}
   \right]</script><p>对于任意一个对角块 $L_i$, 它也就对应了第 $i$ 个子图 $A_i$ (假设顶点个数为$n_i$)的拉普拉斯矩阵。由于 $A_i$ 是全连通的，其连通分量等于其$0$特征值的重数$1$，对应的特征向量也即是指示向量 $\boldsymbol1$, 其余$n-n_i$维用$0$填充。</p>
<h4 id="0-4-Cut"><a href="#0-4-Cut" class="headerlink" title="0.4 Cut"></a>0.4 Cut</h4><p>对于无向图 $G(V,E)$, 我们将它划分成互相不连通的 $k$ 个子图，每个子图的顶点集为：$A_1,A_1,\dots,A_k$, 满足 $A_i \cap A_j = \emptyset, \cup_{i=1}^k A_i=V$ </p>
<p>对于任意两个子图的点集 $A，B \in V, A \cap B = \emptyset$, 定义 $A, B$  之间的权重为</p>
<script type="math/tex; mode=display">
W(A,B) = \sum\limits_{i\in A,i\in B}w_{ij}</script><p>那么对于 $V$ 中互相不连通的 $k$ 个子图，我们定义 $G$ 上的 <strong>cut</strong> </p>
<script type="math/tex; mode=display">
cut(A_1,A_2,\dots,A_k) = \frac{1}{2}\sum\limits_{i=1}^kW(A_i, \bar A_i)</script><h4 id="0-5-形式化Cut"><a href="#0-5-形式化Cut" class="headerlink" title="0.5 形式化Cut"></a>0.5 形式化Cut</h4><p>对于第 $t$ 个子图，引入<strong>指示向量</strong> $f^t \in R^n$ , 用来指示属于第 $t$ 个子图的顶点:</p>
<script type="math/tex; mode=display">
f^t_j=
\begin{cases}
1, \quad & j \in A_t \\
0, \quad & else
\end{cases}</script><p>那么有</p>
<script type="math/tex; mode=display">
\begin{align}
(f^t)^TLf^t
& = \frac{1}{2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n w_{ij}(f^t_i-f^t_j)^2 \\
& = \frac{1}{2}(\sum\limits_{i \in A_t, j \notin A_t}w_{ij} + \sum\limits_{i \notin A_t, j \in A_t}w_{ij} )\\
& = \frac{1}{2}(W(A_t,\bar A_t)+W(\bar A_t,A_t)) \\
& = W(A_t,\bar A_t) \\
\end{align}</script><p>于是我们就有了形式化表示两个子图间权重的数学表达式：</p>
<script type="math/tex; mode=display">
W(A_t,\bar A_t)=(f^t)^TLf^t</script><p>于是有</p>
<script type="math/tex; mode=display">
\begin{align}
cut(A_1,A_2,\dots,A_k) 
& = \frac{1}{2}\sum\limits_{t=1}^kW(A_t, \bar A_t) \\
& = \frac{1}{2}\sum\limits_{t=1}^k (f^t)^TLf^t \\
& = \frac{1}{2}Tr(F^TLF)
\end{align}</script><p>不失一般性，我们使用行变换将 $F$ 重排列成<strong>阶梯型矩阵</strong>，为了维持<strong>cut</strong>值不变，相应的 $L$ 也要按照乘法运算规则重排。重排后目标函数的意义就是对 $L$ 中的 $k$ 个对角块之和，第$i$ 个对角块的size为 $m \times m$, $m$ 为 $f_i\boldsymbol1$ 。</p>
<h3 id="1-MinCut"><a href="#1-MinCut" class="headerlink" title="1. MinCut"></a>1. MinCut</h3><p>对于一个聚类问题，我们的目的就是希望将数据集 $X \in R^{d \times n}$ 分割成 $k$ 个子集，每一个子集表示一类数据。根据不同的分割准则，可以引出不同的聚类算法。</p>
<ul>
<li><p>通过<strong>最小化每一个子集的类内散度</strong>，我们可以引出传统的<strong>KMeans</strong></p>
</li>
<li><p>下面，我们通过另一种准则：<strong>每一个子集之间的权重尽量小</strong>，引出<strong>Min-cut</strong></p>
</li>
</ul>
<p>前面我们已经知道，<strong>cut</strong> 就是用来衡量分割后每一个子集间的权重的，于是就很自然的可以用<strong>Min-cut</strong>的思想来描述这一准则：</p>
<script type="math/tex; mode=display">
\min\limits_{F \in Ind} cut(A_1,A_2,\dots,A_k) => \min\limits_{F \in Ind} Tr(F^TLF)</script><p>然而，该问题存在<strong>Trivial Solution</strong>。因为约束$F \in Ind$ 只能限制每一个样本仅属于一类，无法保证最终一定出现$k$ 个类别。所以可能出现所有样本都聚在一个集合中的情况，$Tr(F^TLF)=Tr(f^tLf^t)=Tr(\boldsymbol1^TF\boldsymbol1)=0$。</p>
<p><strong>题外话，需要注意的是，虽然有Trivial Solution, 但是在实验中我发现算法并不一定就会跑到这个解上。</strong></p>
<p>为了解决这个问题，直觉上来说，我们需要在上述的约束$F \in Ind$ 之上，再添加上一些约束，保证​在分割后的 $k$ 个集合都能存有顶点。那么一个很直接的操作就是加上区间限制：</p>
<script type="math/tex; mode=display">
\min\limits_{F \in Ind;a \leq F^T \boldsymbol 1 \leq b} Tr(F^TLF)</script><p>这样就能够将每一个类别的样本数目bound在 $[a, b]$, 于是就避免了<strong>Trivial</strong>的情况出现。</p>
<p>但是这么做，在实操中发现行不通。下面使用<strong>Coordinate Descend</strong> ，对每一个<strong>样本</strong>分别求解。</p>
<p>令 $F=[f_1 \  f_2 \  \dots \  f_n]^T$, 则有</p>
<script type="math/tex; mode=display">
\begin{align}
Tr(F^TLF) &= 
Tr
\left(

\left[
    \begin{matrix}
    f_1 & \dots & f_i & \dots & f_n
    \end{matrix}
\right]

\left[
   \begin{matrix}
   L_{11} & \dots & L_{1i} & \dots & L_{1n} \\
   \vdots & \ddots & \vdots & \ddots & \vdots  \\
   L_{i1} & \dots & L_{ii} & \dots & L_{in} \\
   \vdots & \ddots & \vdots & \ddots & \vdots  \\
   L_{n1} & \dots & L_{ni} & \dots & L_{nn} \\
   \end{matrix}
\right]

\left[
   \begin{matrix}
   f_1^T  \\
   \vdots \\
   f_i^T  \\
   \vdots \\
   f_n^T \\
   \end{matrix}
\right]

\right)

\\
& = 
Tr
\left(

\left[
    \begin{matrix}
    f_i & \dots & f_1 & \dots & f_n
    \end{matrix}
\right]

\left[
   \begin{matrix}
   L_{ii} & \dots & L_{i1} & \dots & L_{in} \\
   \vdots & \ddots & \vdots & \ddots & \vdots  \\
   L_{1i} & \dots & L_{11} & \dots & L_{1n} \\
   \vdots & \ddots & \vdots & \ddots & \vdots  \\
   L_{ni} & \dots & L_{n1} & \dots & L_{nn} \\
   \end{matrix}
\right]

\left[
   \begin{matrix}
   f_i^T  \\
   \vdots \\
   f_1^T  \\
   \vdots \\
   f_n^T \\
   \end{matrix}
\right]

\right)
\\
&=Tr
\left( 
    \left[
        \begin{matrix}
            f_i & F_i 
        \end{matrix}
    \right]

    \left[
        \begin{matrix}
            L_{ii} & L_i^T \\
            L_{i} & L_o
        \end{matrix}
    \right]

    \left[
        \begin{matrix}
            f_i^T \\
            F_i^T
        \end{matrix}
    \right]
\right)
\\
&=Tr
\left(
    f_iL_{ii}f_i^T+f_iL_i^TF_i^T+F_iL_if_i^T+F_iL_oF_i^T
\right)
\\
&=Tr
\left(
    f_iL_{ii}f_i^T+2F_iL_if_i^T+F_iL_oF_i^T
\right)\ (\boldsymbol{Tr(A)=Tr(A^T)})

\end{align}</script><p>回到开始的优化问题</p>
<script type="math/tex; mode=display">
\begin{align}
    & \min\limits_{F \in Ind;a \leq F^T \boldsymbol 1 \leq b} Tr(F^TLF) \\
    &=> \min\limits_{f_i \in Ind;a \leq F^T \boldsymbol 1 \leq b}
    Tr(f_iL_{ii}f_i^T+2F_iL_if_i^T+F_iL_oF_i^T) \\
    &=> \min\limits_{f_i \in Ind;a \leq F^T \boldsymbol 1 \leq b}
    Tr(F_iL_if_i^T) \\
    &=> \min\limits_{f_i \in Ind;a \leq F^T \boldsymbol 1 \leq b}
    f_i^T(F_iL_i)\ (\boldsymbol{Tr(ab^T)=b^Ta=a^Tb})

\end{align}</script><p>所以 $f_i$ 的更新规则是指示 $k$ 维向量 $F_iL_i$ 中最小的那个元素的位置。$F_i\in R^{k \times n-1}$ 是除了第 $i$ 个样本之外的指示矩阵的转置，$L_i$ 是 $L$ 的第 $i$ 列。使用列变换将 $F_i$ 转换成阶梯型矩阵，$L$ 也相应地变换，得到如下图。</p>
<p><img src="/2019/11/28/经典无监督方法/Spectral.svg" alt="DSC"></p>
<p>其中黑线即表示 $L_i$，红点对应需要更新的样本点 $i$。那么 $F_iL_i$ 的结果就是四个值，分别是四个椭圆包裹的黑线值之和。如若把 $L$ 看成样本之间的<strong>不相似性</strong>，那么这四个值即意味着第 $i$ 个样本与4个类簇的不相似性，结果就是希望样本分配到不相似性最小、也即相似性最大的那个类簇。</p>
<p>但是当某一个对角块很大时，意味着椭圆所包裹的值将越多，其总和也将大概率更小。因为 $L$ 中除了对角元，其余都是负数。负数的值越多，大概率总值就越小。此时当某一样本更新时，那么它大概率会分配进大的对角块。这就形成了一种类似黑洞的现象。虽然有根绳子拉住了样本不掉进去(bound约束)，但是黑洞里的值却很难再逃逸出来。</p>
<p>从上面的分析中，我们点出了一个很关键的东西，就是 $L$ 可以被看成描述样本间不相识性的一种特例。但是作为一种不相似矩阵， $L$ 由于其出对角线外其余元素非正，对于<strong>Coordinate Descend</strong>不太友好。于是这样我们可以扩展一下，<strong>对 $L$ 进行改造或者说寻找更work的不相识矩阵，或者相似矩阵并最大化</strong>。</p>
<h3 id="2-RCut"><a href="#2-RCut" class="headerlink" title="2. RCut"></a>2. RCut</h3><p>由于<strong>MinCut</strong>存在<strong>Trivial Solution</strong>，上面我在<strong>MinCut</strong>的基础上加上了bound策略，效果并不理想。我们依旧希望样本在划分时，可以考虑到将被划分到的类簇的均衡情况。考虑每一个类簇样本数目均衡，下面引出<strong>RCut</strong>:</p>
<script type="math/tex; mode=display">
Rcut(A_1,A_2,\dots,A_k)=\frac{1}{2}\sum\limits_{i=1}^k \frac{W(A_i,\bar{A_i})}{|A_i|}</script><p>如果按照<strong>MinCut</strong>中 $f$ 的定义，那么 $|A_t|=(f^t)^Tf^t$; 另外根据 $W(A_t,\bar A_t)=(f^t)^TLf^t$, 我们很容易就能得到</p>
<script type="math/tex; mode=display">
Rcut(A_1,A_2,\dots,A_k)=\frac{1}{2}\sum\limits_{t=1}^k \frac{(f^t)^TLf^t}{(f^t)^Tf^t}</script><p>为了方便化简，我们希望$(f^t)^Tf^t=1$, 那么很自然想到的操作就是修改 $f$ 的定义。因为修改$f$ 并不改变指示能力。</p>
<script type="math/tex; mode=display">
f^t_j=
\begin{cases}
\frac{1}{\sqrt{|A_t|}}, \quad & j \in A_t \\
0, \quad & else
\end{cases}</script><p>如此一来，我们得到</p>
<script type="math/tex; mode=display">
Rcut(A_1,A_2,\dots,A_k)=\frac{1}{2}\sum\limits_{i=1}^k (f^t)^TLf^t=\frac{1}{2}Tr(F^TLF)</script><p>它表示的不再是 $L$ 中 $k$ 个对角块的和，而是加权求和，第 $t$ 个对角块的权重是 $\frac{1}{|A_t|}$。这样在样本更新时，就不再是容易地被划分到大的对角块内，因为大的对角块对应的权重就越小，这样就起到了一部分均衡作用。</p>
<p>但是该问题并不好解决，于是人们就想，把这个离散的问题连续化：</p>
<script type="math/tex; mode=display">
\min\limits_{F^TF=I} Tr(F^TLF)</script><p>因为连续化之后问题变得相当简单。后续通过KMeans再补救回去。</p>
<h3 id="3-NCut"><a href="#3-NCut" class="headerlink" title="3. NCut"></a>3. NCut</h3><p>考虑每一个类簇的度均衡，</p>
<script type="math/tex; mode=display">
Ncut(A_1,A_2,\dots,A_k)=\frac{1}{2}\sum\limits_{i=1}^k \frac{W(A_i,\bar{A_i})}{vol(A_i)}</script><p>$vol(A_t)=\sum\limits_{i \in A_t} d_i$</p>
<p>$f^TDf = \sum\limits_{i=1}^nf_i^2d_i=\sum\limits_{i \in A_t}d_i = vol(A_t)$</p>
<script type="math/tex; mode=display">
Ncut(A_1,A_2,\dots,A_k)=\frac{1}{2}\sum\limits_{t=1}^k \frac{(f^t)^TLf^t}{(f^t)^TDf^t}</script><p>同样为了方便化简，我们希望 $(f^t)^TDf^t=1$, 所以有</p>
<script type="math/tex; mode=display">
f^t_j=
\begin{cases}
\frac{1}{\sqrt{vol(A_t)}}, \quad & j \in A_t \\
0, \quad & else
\end{cases}</script><p>于是</p>
<script type="math/tex; mode=display">
Ncut(A_1,A_2,\dots,A_k)=\frac{1}{2}\sum\limits_{t=1}^k (f^t)^TLf^t=\frac{1}{2}Tr(F^TLF)</script><p>同样，该问题也不好求解，故连续化成如下求解：</p>
<script type="math/tex; mode=display">
\min\limits_{F^TDF=I} Tr(F^TLF)</script><p>令$F=D^{-1/2}H$ , 那么 </p>
<script type="math/tex; mode=display">
F^TDF=H^TD^{-1/2}DD^{-1/2}H=H^TH=I \\
F^TLF=H^TD^{-1/2}LD^{-1/2}H</script><p>令 $\hat{L} = D^{-1/2}LD^{-1/2}=I-D^{-1/2}AD^{-1/2}$, 有</p>
<script type="math/tex; mode=display">
\min\limits_{H^TH=I} Tr(H^T\hat{L}H) => \min\limits_{F^TF=I} Tr(F^T\hat{L}F)</script><p>$\hat L$ 也叫<strong>标准化Laplacian Matrix</strong>, $\hat L_{ij}=\frac{L_{ij}}{\sqrt{d_i\times d_j}}$</p>
]]></content>
      <categories>
        <category>Theory</category>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Unsupervised</tag>
      </tags>
  </entry>
  <entry>
    <title>HNSW</title>
    <url>/2019/04/02/HNSW/</url>
    <content><![CDATA[<p>ref: Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs </p>
]]></content>
      <categories>
        <category>ANN</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>HNSW</tag>
      </tags>
  </entry>
  <entry>
    <title>ITQ</title>
    <url>/2019/03/13/ITQ/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>ref: Iterative Quantization: A Procrustean Approach to Learning Binary Codes</p>
<h2 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h2><script type="math/tex; mode=display">\min\limits_{B, R}\left\| B-XWR \right\|_F^2</script><p>其中 $R \in R^{c \times c}​$ 是旋转矩阵，$B \in R^{n \times c}​$ 是超立方体的顶点坐标, $W \in R^{d \times c}​$ 表示投影矩阵；</p>
<p><em>Step1.</em> 将数据集先用PCA等降维方法将数据从原始 $d​$ 维空间降维($W​$)至 $c​$ 维空间；</p>
<p><em>Step2.</em> 对 $c$ 维空间中的样本点进行旋转($R$)，同时将旋转后的样本点映射到 $c$ 维空间某一立方体的顶点( $B$, 共 $2^c$ 个顶点)  ；</p>
<p><em>Step3.</em> 迭代更新<em>Step2</em> 中的$R\&amp;B​$ , 使得映射之后的量化误差最小。</p>
<h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>$W$ 能否自己学？</p>
]]></content>
      <categories>
        <category>ANN</category>
        <category>Quantization</category>
      </categories>
      <tags>
        <tag>Procrustean</tag>
        <tag>Binary Code</tag>
        <tag>Similarity Preserving</tag>
      </tags>
  </entry>
  <entry>
    <title>IVFPQ</title>
    <url>/2019/03/12/IVFPQ/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>ref: Production quantization for nearest neighbor search</p>
]]></content>
      <categories>
        <category>ANN</category>
        <category>Quantization</category>
        <category>PQ</category>
      </categories>
      <tags>
        <tag>VQ</tag>
        <tag>PQ</tag>
        <tag>IVFADC</tag>
      </tags>
  </entry>
  <entry>
    <title>Approximate Nearest Neighbor Search 初探</title>
    <url>/2019/03/04/ANN/</url>
    <content><![CDATA[<h2 id="Approximate-Nearest-Neighbor-Search-初探"><a href="#Approximate-Nearest-Neighbor-Search-初探" class="headerlink" title="Approximate Nearest Neighbor Search 初探"></a>Approximate Nearest Neighbor Search 初探</h2><p>对于一个检索任务：给定一个<em>query</em> $q \in R^d$，需要在数据集 $X \in R^{N \times D}$ 中找到距离 $q$ 最近的样本，距离度量函数 $d$ 一般使用欧式距离。类似地可以扩展到寻找 $K$ 近邻。</p>
<script type="math/tex; mode=display">NN(q) = \arg \min\limits_{x \in X} d(q, x)</script><p>最朴素的办法,我们可以使用 <strong><em>Exact Search</em></strong>，这样一定能准确地找到离 $q$ 最近的样本。但由于 <em>Exact Search</em> 的复杂度是 $O(ND)$，空间复杂度 $O(ND)$，而实际的应用场景中往往需要在<strong>毫秒级</strong>的响应时间，数据规模稍大（million）的情况下便无法应用上。</p>
<p>ANN主要思想是通过<strong>损失一部分检索精度来最大限度地换取检索速度的提升</strong>。目前主流的方法可分为以下大致4类：</p>
<ol>
<li>Tree: KD-Tree等；</li>
<li>Hash: LSH等；</li>
<li>Quantization: PQ, OPQ, LOPQ等；</li>
<li>Graph: HNSW, NSG等;</li>
<li>auxiliary：IMI等。</li>
</ol>
<p>其中Tree、Hash和Quantization Based的方法是基于空间划分的思想，将空间划分成<em>cell/Voronoi</em>，然后利用上某些辅助的索引结构，可以在检索的过程中起到同时降低 $N$ 和 $D$ 来减少计算复杂度，如IVFPQ(PQ + Inverted Index)。</p>
<p>Graph Based的方法一般需要构建 $K$ 近邻图，比起其他方法更占空间资源，但是检索精度一般更高。检索过程一般是通过降低 $N$ 来减少计算法复杂度。</p>
]]></content>
      <categories>
        <category>ANN</category>
      </categories>
      <tags>
        <tag>ANN</tag>
      </tags>
  </entry>
</search>
